{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AMoQmkXZLP2M3XdqeMPKSYwX7M0lOxkw",
      "authorship_tag": "ABX9TyOVwPwFeQgp447m+w+BWSu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tekleab15/N-gram-Language-Models/blob/main/N_gram_language_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PXzeNOTAbqKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import re, csv\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/Amharic_Corpus/Copy_of_GPAC.txt'\n",
        "\n",
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^ሀ-ፐ0-9\\s\\-\\.,!?]', '', text)\n",
        "def create_ngrams(tokens, n):\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_CiZX9pcSCi",
        "outputId": "f473a49b-b40e-4e2b-eb7b-de0e90d1138a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams = []\n",
        "bigrams = []\n",
        "trigrams = []\n",
        "fourgrams = []\n",
        "# Read and process the file line by line\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        cleaned_line = clean_text(line)\n",
        "        tokens = cleaned_line.split()\n",
        "        # Generate unigrams and add to the list\n",
        "        unigrams.extend(create_ngrams(tokens, 1))\n",
        "        bigrams.extend(create_ngrams(tokens, 2))\n",
        "        trigrams.extend(create_ngrams(tokens, 3))\n",
        "        fourgrams.extend(create_ngrams(tokens, 4))\n",
        "        # Print progress (optional)\n",
        "        if len(unigrams) % 1000 == 0:\n",
        "            print(f\"Processed {len(unigrams)} unigrams...\")\n",
        "\n",
        "print(\"Unigram: \",unigrams[:10])\n",
        "print(\"Bigram: \",bigrams[:10])\n",
        "print(\"Trigram: \",trigrams[:10])\n",
        "print(\"Fourgram: \",fourgrams[:10])"
      ],
      "metadata": {
        "id": "D8z3Jv5dRKHv",
        "outputId": "aebf3262-0b23-4871-af69-f48f62ad3ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram:  ['ምን', 'መሰላችሁ?', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም']\n",
            "Bigram:  ['ምን መሰላችሁ?', 'መሰላችሁ? አንባቢያን', 'አንባቢያን ኢትዮጵያ', 'ኢትዮጵያ በተደጋጋሚ', 'በተደጋጋሚ ጥሪው', 'ጥሪው ደርሷት', 'ደርሷት ልትታደመው', 'ልትታደመው ያልቻለችው', 'ያልቻለችው የአለም', 'የአለም የእግር']\n",
            "Trigram:  ['ምን መሰላችሁ? አንባቢያን', 'መሰላችሁ? አንባቢያን ኢትዮጵያ', 'አንባቢያን ኢትዮጵያ በተደጋጋሚ', 'ኢትዮጵያ በተደጋጋሚ ጥሪው', 'በተደጋጋሚ ጥሪው ደርሷት', 'ጥሪው ደርሷት ልትታደመው', 'ደርሷት ልትታደመው ያልቻለችው', 'ልትታደመው ያልቻለችው የአለም', 'ያልቻለችው የአለም የእግር', 'የአለም የእግር ኳስ']\n",
            "Fourgram:  ['ምን መሰላችሁ? አንባቢያን ኢትዮጵያ', 'መሰላችሁ? አንባቢያን ኢትዮጵያ በተደጋጋሚ', 'አንባቢያን ኢትዮጵያ በተደጋጋሚ ጥሪው', 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት', 'በተደጋጋሚ ጥሪው ደርሷት ልትታደመው', 'ጥሪው ደርሷት ልትታደመው ያልቻለችው', 'ደርሷት ልትታደመው ያልቻለችው የአለም', 'ልትታደመው ያልቻለችው የአለም የእግር', 'ያልቻለችው የአለም የእግር ኳስ', 'የአለም የእግር ኳስ ዋ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_ngram_counts_to_disk(file_path, n, output_file):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f, open(output_file, 'w', newline='', encoding='utf-8') as out_f:\n",
        "        writer = csv.writer(out_f)\n",
        "        chunk_size = 1000\n",
        "        lines = []\n",
        "\n",
        "        for i, line in enumerate(f):\n",
        "            cleaned_line = clean_text(line)\n",
        "            tokens = cleaned_line.split()\n",
        "            lines.append(tokens)\n",
        "\n",
        "            if (i + 1) % chunk_size == 0:\n",
        "                ngram_counts = {}\n",
        "                for tokens in lines:\n",
        "                    ngrams = create_ngrams(tokens, n)\n",
        "                    for ngram in ngrams:\n",
        "                        ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "                writer.writerows(ngram_counts.items())\n",
        "                lines = []\n",
        "                print(f\"Processed {i + 1} lines...\")\n",
        "\n",
        "        if lines:\n",
        "            ngram_counts = {}\n",
        "            for tokens in lines:\n",
        "                ngrams = create_ngrams(tokens, n)\n",
        "                for ngram in ngrams:\n",
        "                    ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "            writer.writerows(ngram_counts.items())\n",
        "\n",
        "# Write n-gram counts to disk\n",
        "write_ngram_counts_to_disk(file_path, 1, 'unigrams.csv')\n",
        "write_ngram_counts_to_disk(file_path, 2, 'bigrams.csv')\n",
        "write_ngram_counts_to_disk(file_path, 3, 'trigrams.csv')\n",
        "write_ngram_counts_to_disk(file_path, 4, 'fourgrams.csv')\n"
      ],
      "metadata": {
        "id": "4TDud4JfPaLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_total_counts(file_path):\n",
        "    total_count = 0\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            total_count += int(row[1])\n",
        "    return total_count\n",
        "\n",
        "total_unigrams = calculate_total_counts('unigrams.csv')\n",
        "total_bigrams = calculate_total_counts('bigrams.csv')\n",
        "total_trigrams = calculate_total_counts('trigrams.csv')\n",
        "total_fourgrams = calculate_total_counts('fourgrams.csv')\n",
        "\n",
        "print(f\"Total unigrams: {total_unigrams}\")\n",
        "print(f\"Total bigrams: {total_bigrams}\")\n",
        "print(f\"Total trigrams: {total_trigrams}\")\n",
        "print(f\"Total fourgrams: {total_fourgrams}\")\n",
        "\n",
        "def calculate_probabilities_from_disk(file_path, total_count):\n",
        "    ngram_prob = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            ngram, count = row\n",
        "            ngram_prob[ngram] = int(count) / total_count\n",
        "    return ngram_prob\n",
        "\n",
        "unigram_prob = calculate_probabilities_from_disk('unigrams.csv', total_unigrams)\n",
        "bigram_prob = calculate_probabilities_from_disk('bigrams.csv', total_bigrams)\n",
        "trigram_prob = calculate_probabilities_from_disk('trigrams.csv', total_trigrams)\n",
        "fourgram_prob = calculate_probabilities_from_disk('fourgrams.csv', total_fourgrams)\n",
        "\n"
      ],
      "metadata": {
        "id": "A4FUgQyKPrCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1: Create n-grams for n=1, 2, 3, 4. You can show sample prints."
      ],
      "metadata": {
        "id": "RRhgTvDkfCDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Count the freuency of n-grams"
      ],
      "metadata": {
        "id": "AFIiPS7KJHcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memor efficient technique\n",
        "import csv\n",
        "def write_ngram_counts_to_disk(file_path, n, output_file):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f, open(output_file, 'w', newline='', encoding='utf-8') as out_f:\n",
        "        writer = csv.writer(out_f)\n",
        "        ngram_counts = {}\n",
        "        total_count = 0\n",
        "        chunk_size = 1000\n",
        "        lines = []\n",
        "\n",
        "        for i, line in enumerate(f):\n",
        "            cleaned_line = clean_text(line)\n",
        "            tokens = cleaned_line.split()\n",
        "            lines.append(tokens)\n",
        "\n",
        "            if (i + 1) % chunk_size == 0:\n",
        "                for tokens in lines:\n",
        "                    ngrams = create_ngrams(tokens, n)\n",
        "                    for ngram in ngrams:\n",
        "                        ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "                        total_count += 1\n",
        "                writer.writerows(ngram_counts.items())\n",
        "                ngram_counts.clear()\n",
        "                lines = []\n",
        "                print(f\"Processed {i + 1} lines...\")\n",
        "\n",
        "        if lines:\n",
        "            for tokens in lines:\n",
        "                ngrams = create_ngrams(tokens, n)\n",
        "                for ngram in ngrams:\n",
        "                    ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "                    total_count += 1\n",
        "            writer.writerows(ngram_counts.items())\n",
        "            ngram_counts.clear()\n",
        "\n",
        "    return total_count\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Amharic_Corpus/Copy_of_GPAC.txt'\n",
        "\n",
        "total_unigrams = write_ngram_counts_to_disk(file_path, 1, 'unigrams.csv')\n",
        "total_bigrams = write_ngram_counts_to_disk(file_path, 2, 'bigrams.csv')\n",
        "total_trigrams = write_ngram_counts_to_disk(file_path, 3, 'trigrams.csv')\n",
        "total_fourgrams = write_ngram_counts_to_disk(file_path, 4, 'fourgrams.csv')\n",
        "\n",
        "print(f\"Total unigrams: {total_unigrams}\")\n",
        "print(f\"Total bigrams: {total_bigrams}\")\n",
        "print(f\"Total trigrams: {total_trigrams}\")\n",
        "print(f\"Total fourgrams: {total_fourgrams}\")\n"
      ],
      "metadata": {
        "id": "IdpLfgYTOEY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
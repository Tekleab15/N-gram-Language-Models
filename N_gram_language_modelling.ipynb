{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AMoQmkXZLP2M3XdqeMPKSYwX7M0lOxkw",
      "authorship_tag": "ABX9TyPQgkdr2gLhv7u9u6YSiySj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tekleab15/N-gram-Language-Models/blob/main/N_gram_language_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PXzeNOTAbqKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "import re, csv\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/Amharic_Corpus/Copy_of_GPAC.txt'\n",
        "\n",
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^ሀ-ፐ0-9\\s\\-\\.,!?]', '', text)\n",
        "def create_ngrams(tokens, n):\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_CiZX9pcSCi",
        "outputId": "33478ca4-3450-4e5e-ee97-4f5695a9ca9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints*"
      ],
      "metadata": {
        "id": "E2kFYUPEnbUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize defaultdict for n-grams and their frequencies\n",
        "unigram_freq = defaultdict(int)\n",
        "bigram_freq = defaultdict(int)\n",
        "trigram_freq = defaultdict(int)\n",
        "fourgram_freq = defaultdict(int)\n",
        "\n",
        "# Generator function to read and clean lines from the file\n",
        "def read_and_clean_lines(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            cleaned_line = clean_text(line)\n",
        "            tokens = cleaned_line.split()\n",
        "            if tokens:\n",
        "                yield tokens\n",
        "\n",
        "# Initialize defaultdict for n-grams and their frequencies\n",
        "unigram_freq = defaultdict(int)\n",
        "bigram_freq = defaultdict(int)\n",
        "trigram_freq = defaultdict(int)\n",
        "fourgram_freq = defaultdict(int)\n",
        "\n",
        "# Process the file using the generator function\n",
        "for tokens in read_and_clean_lines(file_path):\n",
        "    for ngram in create_ngrams(tokens, 1):\n",
        "        unigram_freq[ngram] += 1\n",
        "    for ngram in create_ngrams(tokens, 2):\n",
        "        bigram_freq[ngram] += 1\n",
        "    for ngram in create_ngrams(tokens, 3):\n",
        "        trigram_freq[ngram] += 1\n",
        "    for ngram in create_ngrams(tokens, 4):\n",
        "        fourgram_freq[ngram] += 1\n",
        "\n",
        "# Print sample n-grams with their frequencies\n",
        "print(\"Unigram: \", list(unigram_freq.items())[:10])\n",
        "print(\"Bigram: \", list(bigram_freq.items())[:10])\n",
        "print(\"Trigram: \", list(trigram_freq.items())[:10])\n",
        "print(\"Fourgram: \", list(fourgram_freq.items())[:10])"
      ],
      "metadata": {
        "id": "D8z3Jv5dRKHv",
        "outputId": "acde0e1d-0c93-4ff4-e0ad-51136fcb1309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram:  [('ምን', 43593), ('መሰላችሁ?', 942), ('አንባቢያን', 379), ('ኢትዮጵያ', 28031), ('በተደጋጋሚ', 4223), ('ጥሪው', 127), ('ደርሷት', 10), ('ልትታደመው', 1), ('ያልቻለችው', 24), ('የአለም', 1273)]\n",
            "Bigram:  [('ምን መሰላችሁ?', 328), ('መሰላችሁ? አንባቢያን', 1), ('አንባቢያን ኢትዮጵያ', 1), ('ኢትዮጵያ በተደጋጋሚ', 9), ('በተደጋጋሚ ጥሪው', 1), ('ጥሪው ደርሷት', 1), ('ደርሷት ልትታደመው', 1), ('ልትታደመው ያልቻለችው', 1), ('ያልቻለችው የአለም', 1), ('የአለም የእግር', 5)]\n",
            "Trigram:  [('ምን መሰላችሁ? አንባቢያን', 1), ('መሰላችሁ? አንባቢያን ኢትዮጵያ', 1), ('አንባቢያን ኢትዮጵያ በተደጋጋሚ', 1), ('ኢትዮጵያ በተደጋጋሚ ጥሪው', 1), ('በተደጋጋሚ ጥሪው ደርሷት', 1), ('ጥሪው ደርሷት ልትታደመው', 1), ('ደርሷት ልትታደመው ያልቻለችው', 1), ('ልትታደመው ያልቻለችው የአለም', 1), ('ያልቻለችው የአለም የእግር', 1), ('የአለም የእግር ኳስ', 5)]\n",
            "Fourgram:  [('ምን መሰላችሁ? አንባቢያን ኢትዮጵያ', 1), ('መሰላችሁ? አንባቢያን ኢትዮጵያ በተደጋጋሚ', 1), ('አንባቢያን ኢትዮጵያ በተደጋጋሚ ጥሪው', 1), ('ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት', 1), ('በተደጋጋሚ ጥሪው ደርሷት ልትታደመው', 1), ('ጥሪው ደርሷት ልትታደመው ያልቻለችው', 1), ('ደርሷት ልትታደመው ያልቻለችው የአለም', 1), ('ልትታደመው ያልቻለችው የአለም የእግር', 1), ('ያልቻለችው የአለም የእግር ኳስ', 1), ('የአለም የእግር ኳስ ዋ', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.*"
      ],
      "metadata": {
        "id": "zW0QF72hnjPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating total count of each counts\n",
        "unigram_count = sum(unigram_freq.values())\n",
        "bigram_count = sum(bigram_freq.values())\n",
        "trigram_count = sum(trigram_freq.values())\n",
        "fourgram_count = sum(fourgram_freq.values())\n",
        "\n",
        "# Calculating probablities for each grams\n",
        "def unigramProbablity(unigram):\n",
        "  return unigram_freq[unigram] / unigram_count\n",
        "\n",
        "def bigramProbablity(bigram):\n",
        "  first_word, second_word = bigram.split()[0]\n",
        "  return bigram_freq[bigram] / unigram_freq[first_word]\n",
        "\n",
        "def trigramProbablity(trigram):\n",
        "  first_two_words = \" \".join(trigram.split()[:2])\n",
        "  return trigram_freq[trigram] / bigram_freq[first_two_words]\n",
        "def fourgramProbablity(fourgram):\n",
        "  first_three_words = \" \".join(fourgram.split()[:3])\n",
        "  return fourgram_freq[fourgram] / trigram_freq[first_three_words]\n",
        "\n",
        "# Calculate the probabilities of each grams(top ten)\n",
        "top_ten_unigram = sorted(unigra)"
      ],
      "metadata": {
        "id": "4TDud4JfPaLg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdpLfgYTOEY_"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}